{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "This starts with a version of the Dipy Tracking Quickstart Tutorial (http://nipy.org/dipy/examples_built/tracking_quick_start.html#example-tracking-quick-start) for beginners to python. We then go through two more advanced methods for working with streamline data: Quickbundles (link) and Recobundles (link).\n",
    "\n",
    "NOTE: For this tutorial, we assume you have eddy-corrected (https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/eddy) (and if you can susceptibility-corrected; https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/topup) your data\n",
    "\n",
    "For more examples:\n",
    "http://nipy.org/dipy/examples_built/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and Load our data \n",
    "(if we don't have it already)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.data import fetch_stanford_hardi, read_stanford_hardi\n",
    "fetch_stanford_hardi()\n",
    "img, gtab = read_stanford_hardi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = img.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check and see if our files match our expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our data is 81 x 106 voxel resolution in-plane, 76 slices, and 160 volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gtab.bvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and we have our bvalues... 10 b0 volumes (no diffusion weighting) followed by 150 b2000 volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gtab.bvecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the shape of our bvecs make sense... 160 gradients by 3 dimenstions (xyz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save a B0 image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick the first B0 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "index_of_first_B0 = np.where(gtab.b0s_mask)[0][0]\n",
    "index_of_first_B0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good... we saw from our list that the first volume is a b0 image so this matches our expectation\n",
    "\n",
    "Now let's extract the first volume from our 4-D dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b0_array = data[:,:,:,index_of_first_B0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and save it as a nifti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dipy.io.image import save_nifti\n",
    "\n",
    "save_nifti('ohbm_dipy_b0.nii.gz', b0_array, img.affine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do a quick brain segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the brain segmentation default parameters don't work, so we use specific ones here. On your own data, try the defaults first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dipy.segment.mask import median_otsu\n",
    "\n",
    "maskdata, mask = median_otsu(data, 3, 2, False,\n",
    "                             vol_idx=range(0, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's save the mask to our current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_nifti('ohbm_dipy_mask.nii.gz', mask*1, img.affine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use nilearn plotting to check our mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nilearn.plotting as nip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nip.plot_roi('ohbm_dipy_mask.nii.gz', bg_img='ohbm_dipy_b0.nii.gz', \n",
    "             cmap=nip.cm.bwr_r, alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit a Tensor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dipy.reconst.dti import TensorModel\n",
    "\n",
    "tensor_model = TensorModel(gtab, fit_method='WLS')\n",
    "tensor_fit = tensor_model.fit(data, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate fractional anisotropy (FA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fa = tensor_fit.fa\n",
    "\n",
    "# clip FA values to between 0 and 1\n",
    "fa = np.clip(fa, 0, 1)\n",
    "\n",
    "# save the nifti file\n",
    "save_nifti('ohbm_dipy_fa.nii.gz', fa, img.affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nip.plot_anat('ohbm_dipy_fa.nii.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the tensor fit using the visualization tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.viz import actor, window\n",
    "from dipy.reconst.dti import color_fa\n",
    "from dipy.data import get_sphere\n",
    "\n",
    "ren = window.Renderer()\n",
    "rgb = color_fa(fa, tensor_fit.evecs)\n",
    "    \n",
    "evals = tensor_fit.evals[:,:,28:29]\n",
    "evecs = tensor_fit.evecs[:, :, 28:29]\n",
    "cfa = rgb[:, :, 28:29]\n",
    "cfa /= cfa.max()\n",
    "\n",
    "sphere = get_sphere('symmetric724') # we need a sphere for the render\n",
    "ren.add(actor.tensor_slicer(evals, evecs, scalar_colors=cfa, \n",
    "                                sphere=sphere, scale=0.3))\n",
    "# Save a snapshot as a png\n",
    "window.record(ren, n_frames=1, out_path='ohbm_dipy_ren_ten.png', \n",
    "              size=(600, 600))\n",
    "\n",
    "# Uncomment to show this interactively\n",
    "#window.show(ren)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will display snapshots inline so we can see our results\n",
    "from IPython.display import Image\n",
    "\n",
    "Image('ohbm_dipy_ren_ten.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit a \"higher order\" more complex model\n",
    "\n",
    "Examples of this are :\n",
    "- Constrained Spherical Deconvolution (CSD) \n",
    "- Constant Solid Angle (CSA)/QBall Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we estimate a response function for CSD...\n",
    "\n",
    "The response function is an estimation based on the data of the signal produced by a single coherent bundle of fibers. We will use voxels with very high FA to generate this from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dipy.reconst.csdeconv import ConstrainedSphericalDeconvModel\n",
    "from dipy.reconst.csdeconv import auto_response\n",
    "\n",
    "response, ratio = auto_response(gtab, data, roi_radius=10, fa_thr=0.7)\n",
    "\n",
    "csd_model = ConstrainedSphericalDeconvModel(gtab, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the data and calculate the peaks \n",
    "\n",
    "(we assume these are the dominant fiber directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dipy.direction import peaks_from_model\n",
    "\n",
    "csd_peaks = peaks_from_model(model=csd_model,\n",
    "                             data=data,\n",
    "                             sphere=sphere,\n",
    "                             mask=mask,\n",
    "                             relative_peak_threshold=.5,\n",
    "                             min_separation_angle=25,\n",
    "                             parallel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out our peaks on an axial slice by running this cell... a window will pop up \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make the rendering with the peaks\n",
    "window.clear(ren)\n",
    "ren = window.Renderer()\n",
    "ren.add(actor.peak_slicer(csd_peaks.peak_dirs,\n",
    "                          csd_peaks.peak_values,colors=None))\n",
    "\n",
    "# Uncomment this to interact with the render (a window will pop up)\n",
    "#window.show(ren, size=(900, 900))\n",
    "\n",
    "window.record(ren, out_path='ohbm_dipy_peaks.png', size=(900, 900))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('ohbm_dipy_peaks.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window.clear(ren)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the renders... what kinds of model limitations do we encounter with the tensor model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tractography!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a simple tissue classifier (gray/white/csf) using FA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dipy.tracking.local import ThresholdTissueClassifier\n",
    "\n",
    "tissue_classifier = ThresholdTissueClassifier(fa, 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate seeds to track from in voxels with high FA... Let's seed all of the voxels with FA greater than 0.3 with one seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dipy.tracking.utils import random_seeds_from_mask\n",
    "\n",
    "seeds = random_seeds_from_mask(fa > 0.3, seeds_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dipy.tracking.local import LocalTracking\n",
    "from dipy.tracking.streamline import Streamlines\n",
    "\n",
    "streamline_generator = LocalTracking(csd_peaks, tissue_classifier,\n",
    "                                     seeds, affine=np.eye(4),\n",
    "                                     step_size=0.5)\n",
    "\n",
    "streamlines = Streamlines(streamline_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check how many streamlines we generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(streamlines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at our streamlines now... let's write a function for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_sls(streamlines, out_path='ohbm_dipy_bigcluster.png', interact=False):    \n",
    "    ren = window.Renderer()\n",
    "    \n",
    "    ren.add(actor.line(streamlines))\n",
    "\n",
    "    if interact:\n",
    "        window.show(ren, size=(900, 900))\n",
    "    \n",
    "    window.record(ren, out_path=out_path, size=(900, 900))\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(show_sls(streamlines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save our wholebrain tractography dataset as a .trk file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This streamline dataset contains many streamlines that we are not interested in... there is sometimes a shell of short streamlines around the periphery of the brain that we know are an artifact of our tracking procedure... in this case we just have a few where the mask fit was poor... we can remove them with a length threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the length of each streamline in the dataset\n",
    "from dipy.tracking.utils import length\n",
    "\n",
    "lengths = list(length(streamlines))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig_hist, ax = plt.subplots(1)\n",
    "ax.hist(lengths, color='purple')\n",
    "ax.set_xlabel('Length')\n",
    "ax.set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set a length threshold\n",
    "length_threshold = 20\n",
    "long_streamlines = [] # initialize an empty list\n",
    "for i,sl in enumerate(streamlines):\n",
    "    if lengths[i] > length_threshold:\n",
    "        long_streamlines.append(sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(show_sls(long_streamlines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save our new track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nibabel.streamlines import save as save_trk\n",
    "from nibabel.streamlines import Tractogram\n",
    "\n",
    "save_trk(Tractogram(long_streamlines, affine_to_rasmm=img.affine),\n",
    "         'ohbm_dipy_det_streamlines_long.trk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore our results with Quickbundles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dipy.segment.clustering import QuickBundles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qb = QuickBundles(threshold=6.)\n",
    "clusters = qb.cluster(streamlines)\n",
    "print(len(clusters), \"Clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the centroids..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(show_sls(clusters.centroids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clsz = [len(cl) for cl in clusters]\n",
    "\n",
    "fig_hist, ax = plt.subplots(1)\n",
    "ax.hist(clsz, color='purple')\n",
    "ax.set_xlabel('Length')\n",
    "ax.set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our biggest cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(show_sls(clusters[np.argmax(clsz)], interact=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recobundles\n",
    "\n",
    "Let's try to automatically segment the CST using Recobundles... to do this we need to use the new nibabel streamline API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "template_path_wb = '../OHBM_DIPY_TUTORIAL_DATA/kesh_template_Wholebrain.trk'\n",
    "template_path_bundle = '../OHBM_DIPY_TUTORIAL_DATA/kesh_template_ArcuateL.trk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dipy.align.streamlinear import whole_brain_slr, slr_with_qb, transform_streamlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function that will load a .trk file using the new nibabel streamline API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadtgm_newapi(trkpath):\n",
    "    trkloaded = nib.streamlines.trk.TrkFile.load(trkpath)\n",
    "    hdrloaded = trkloaded.header\n",
    "    tg=trkloaded.tractogram\n",
    "    return tg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our bundle atlas (in this case it's the left Arcuate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "temp_atlasbundle_tg = loadtgm_newapi(template_path_bundle)\n",
    "temp_atlasbundle_sls = temp_atlasbundle_tg.streamlines\n",
    "temp_atlasbundle_as = nib.streamlines.array_sequence.ArraySequence(temp_atlasbundle_sls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll make a function to display streamlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genren(sls, sls2=None, niidata=None, roi1=None, roi2=None, rotx=-90, rotz=90, aff=None, putpath=None, interact=False, orient=True, colors=[(1,1,1)]):\n",
    "    \n",
    "    renderer = window.Renderer()\n",
    "    \n",
    "    if len(colors)>1:\n",
    "        stream_actor = actor.line(sls, colors=colors[1])\n",
    "    else:\n",
    "        stream_actor=actor.line(sls)\n",
    "    stream_actor.RotateX(rotx)\n",
    "    stream_actor.RotateZ(rotz)\n",
    "    renderer.add(stream_actor)\n",
    "    \n",
    "    if sls2 is not None:\n",
    "        stream_actor2 = actor.line(sls2, colors=colors[0])\n",
    "        stream_actor2.RotateX(rotx)\n",
    "        stream_actor2.RotateZ(rotz)\n",
    "        renderer.add(stream_actor2)\n",
    "    \n",
    "    if roi1 is not None and roi2 is not None:\n",
    "        \n",
    "        contour_actor1 = actor.contour_from_roi(roi1, affine=aff, color=(1., 1., 0.),\n",
    "                                          opacity=0.5)\n",
    "        contour_actor2 = actor.contour_from_roi(roi2, affine=aff, color=(1., 0., 0.),\n",
    "                                          opacity=0.5)\n",
    "        renderer.add(contour_actor1)\n",
    "        renderer.add(contour_actor2)\n",
    "        \n",
    "    \n",
    "    if niidata is not None:\n",
    "        slice_actor = actor.slicer(niidata, affine=aff)\n",
    "        renderer.add(slice_actor)\n",
    "\n",
    "    #renderer.set_camera(position=(-176.42, 118.52, 128.20),\n",
    "    #                    focal_point=(113.30, 128.31, 76.56),\n",
    "    #                    view_up=(0.18, 0.00, 0.98))\n",
    "\n",
    "    if orient:\n",
    "        axes_actor = actor.axes(scale=(10, 10, 10), colorx=(1, 0, 0), colory=(0, 1, 0), colorz=(0, 0, 1), opacity=1)\n",
    "        #Create an actor with the coordinate’s system axes where red = x, green = y, blue = z.\n",
    "        renderer.add(axes_actor)\n",
    "\n",
    "    if interact:\n",
    "        window.show(renderer, size=(600, 600), reset_camera=False)\n",
    "    if putpath is not None:\n",
    "        window.record(renderer, out_path=putpath, size=(600, 600))\n",
    "    del renderer\n",
    "    return putpath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display our bundle template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(genren(temp_atlasbundle_sls, putpath='ohbm_dipy_adv_atlasbundle.png', interact=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and display our whole-brain template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_wb_tg = loadtgm_newapi(template_path_wb)\n",
    "temp_wb_sls = temp_wb_tg.streamlines\n",
    "temp_wb_sls_as = nib.streamlines.array_sequence.ArraySequence(temp_wb_sls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(genren(temp_wb_sls, putpath='ohbm_dipy_adv_atlaswb.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the wholebrain streamline dataset we generated earlier in the tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_wb_tg = loadtgm_newapi('ohbm_dipy_det_streamlines_long.trk')\n",
    "sub_wb_sls = sub_wb_tg.streamlines\n",
    "sub_wb_sls_as = nib.streamlines.array_sequence.ArraySequence(sub_wb_sls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a function to roughly register the template-space wholebrain dataset and the subject-space wholebrain dataset we want to extract the Arcuate from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rough_reg(sub_fixed, temp_moving):\n",
    "    #template moves to the subject space\n",
    "    moved, transform, qb_centroids1, qb_centroids2 = whole_brain_slr(sub_fixed, temp_moving, \n",
    "                                                                     verbose=True,\n",
    "                                                                     progressive=True)\n",
    "    return moved, transform, qb_centroids1, qb_centroids2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moved_temp2sub, xfm_temp2sub, qbc1_temp2sub, qbc2_temp2sub = rough_reg(sub_wb_sls_as, temp_wb_sls_as)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(genren(sub_wb_sls, temp_atlasbundle_sls, putpath='pre.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obviously this is incorrect; we need to either\n",
    "- apply the transform we calculated to the streamline template to bring the template into the patient's space\n",
    "- apply the inverse transform to the patient's wholebrain streamlines to bring the patient into the template space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the Atlas bundle template into the subject space using our precomputed affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_atlasbundle_sls_subspace = temp_atlasbundle_tg.copy().apply_affine(xfm_temp2sub).streamlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying registration we can see that the Arcuate bundle template is in the right space (subject-space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(genren(sub_wb_sls, temp_atlasbundle_sls_subspace, putpath='post.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a function to execute recobundles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dipy.segment.bundles import RecoBundles\n",
    "from dipy.segment.clustering import qbx_and_merge\n",
    "\n",
    "def run_rb(templatesls, bucketosls, cluster_map=None, pruning_thr=10):\n",
    "    # try pruning thresh 10 if not specific drop to 5\n",
    "    if cluster_map is None:\n",
    "        cluster_map = qbx_and_merge(bucketosls, \n",
    "                                    thresholds=[40, 25, 20, 10])\n",
    "    else:\n",
    "        print(\"Loading provided cluster map\")\n",
    "\n",
    "    rb = RecoBundles(bucketosls, cluster_map=cluster_map, clust_thr=5)\n",
    "    recognized_atlassp, rec_labels, recognized_ptsp = rb.recognize(model_bundle=templatesls,\n",
    "                                                         model_clust_thr=5.,\n",
    "                                                         reduction_thr=10, pruning_thr=pruning_thr)\n",
    "    return recognized_ptsp, cluster_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb_recognized_bundle, cluster_map = run_rb(temp_atlasbundle_sls_subspace, sub_wb_sls, pruning_thr=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(genren(rb_recognized_bundle, putpath='recobundles.png', interact=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now try playing with the pruning threshold in run_rb. What does this do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
